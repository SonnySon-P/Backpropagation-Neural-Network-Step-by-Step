{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jr05nu_KvIn2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# 權重初始化\n",
        "w1 = 0.11\n",
        "w2 = 0.21\n",
        "w3 = 0.12\n",
        "w4 = 0.08\n",
        "w5 = 0.14\n",
        "w6 = 0.15\n",
        "\n",
        "# 偏置項初始化 (每個神經元都有自己的偏置項)\n",
        "b1 = np.array([0.1, 0.1])  # 第一層的偏置項(對應2個神經元)\n",
        "b2 = np.array([0.05])  # 第二層的偏置項(對應1個神經元)\n",
        "\n",
        "# 輸入值與目標值\n",
        "inputs = np.array([2, 3])\n",
        "target = np.array([1])\n",
        "\n",
        "# 第一層與第二層的權重矩陣\n",
        "L_1_W = np.array([[w1, w3], [w2, w4]])  # 第一層的權重，形狀(2, 2)\n",
        "L_2_W = np.array([[w5], [w6]])  # 第二層的權重，形狀(2, 1)\n",
        "\n",
        "# 學習率、誤差閾值及最大迭代次數\n",
        "a = 0.05\n",
        "set_diff_val = 0.00001\n",
        "now_diff_val = 100\n",
        "iteration = 100\n",
        "\n",
        "# 均方誤差(MSE)損失函數\n",
        "def loss_function(y_pred, y_true):\n",
        "    return 0.5 * np.sum((y_pred - y_true) ** 2)\n",
        "\n",
        "# 迭代過程\n",
        "i = 0\n",
        "while abs(now_diff_val) > set_diff_val and i < iteration:\n",
        "    # 計算第一層的輸出，加入偏置項\n",
        "    h1 = np.dot(inputs, L_1_W) + b1  # 輸入和第一層權重的點積，加上偏置項b1\n",
        "\n",
        "    # 計算第二層的輸出，加入偏置項\n",
        "    prediction = np.dot(h1, L_2_W) + b2  # 輸出層計算，加上偏置項b2\n",
        "\n",
        "    # 計算損失（即誤差）\n",
        "    loss = loss_function(prediction, target)\n",
        "    print(f'第{i + 1}次迭代，估計值是{prediction[0]:.5f}')\n",
        "\n",
        "    # 計算誤差\n",
        "    now_diff_val = prediction - target\n",
        "\n",
        "    # ---- 反向傳播 ----\n",
        "    # 更新第二層的權重\n",
        "    L_2_W[0] = L_2_W[0] - a * h1[0] * now_diff_val  # 更新L_2_W[0]權重\n",
        "    L_2_W[1] = L_2_W[1] - a * h1[1] * now_diff_val  # 更新L_2_W[1]權重\n",
        "\n",
        "    # 更新第二層的偏置項\n",
        "    b2 = b2 - a * now_diff_val  # 更新偏置b2\n",
        "\n",
        "    # 計算第一層的誤差對應的梯度\n",
        "    delta_L1 = np.dot(now_diff_val, L_2_W.T)\n",
        "\n",
        "    # 更新第一層的權重\n",
        "    L_1_W[0][0] = L_1_W[0][0] - a * inputs[0] * delta_L1[0]  # 更新L_1_W[0][0]權重\n",
        "    L_1_W[0][1] = L_1_W[0][1] - a * inputs[0] * delta_L1[1]  # 更新L_1_W[0][1]權重\n",
        "    L_1_W[1][0] = L_1_W[1][0] - a * inputs[1] * delta_L1[0]  # 更新L_1_W[1][0]權重\n",
        "    L_1_W[1][1] = L_1_W[1][1] - a * inputs[1] * delta_L1[1]  # 更新L_1_W[1][1]權重\n",
        "\n",
        "    # 更新第一層的偏置項\n",
        "    b1 -= a * delta_L1  # 更新偏置 b1\n",
        "\n",
        "    # 輸出修正後的權重，逐個輸出每個權重和偏置\n",
        "    print(f'修正後權重 w1 = {L_1_W[0][0]:.2f}, w2 = {L_1_W[1][0]:.2f}, w3 = {L_1_W[0][1]:.2f}, w4 = {L_1_W[1][1]:.2f}, '\n",
        "          f'w5 = {L_2_W[0][0]:.2f}, w6 = {L_2_W[1][0]:.2f}, b1 = {b1[0]:.2f}, b1 = {b1[1]:.2f}, b2 = {b2[0]:.2f}')\n",
        "\n",
        "    i = i + 1"
      ]
    }
  ]
}